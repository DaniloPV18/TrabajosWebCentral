# üï∑Ô∏è TrabajosWeb Central - Plataforma de Web Scraping

Plataforma automatizada multi-servicio para la extracci√≥n de vacantes de portales de empleo y centralizaci√≥n en PostgreSQL. Utiliza Docker Compose para orquestaci√≥n y comunicaci√≥n entre servicios.

---

## üìã Servicios Disponibles

### 1Ô∏è‚É£ **Web Scraper** (Python)
Automatiza la extracci√≥n de vacantes de portales de empleo (HiringRoom) y las centraliza en PostgreSQL. Est√° dise√±ado para ejecutarse de forma aislada mediante contenedores.

#### üöÄ Comandos Principales
| Acci√≥n | Comando |
|--------|---------|
| Construir e Iniciar | `docker-compose up -d --build` |
| Ver Logs (Progreso) | `docker logs -f trabajos_scraper` |
| Ejecuci√≥n Manual | `docker exec -it trabajos_scraper python /app/src/main.py` |
| Detener Servicio | `docker-compose stop scraper` |

#### üõ†Ô∏è Configuraci√≥n del Entorno
El scraper utiliza las siguientes variables definidas en `docker-compose.yml`:
- **DB Host**: `database` (nombre del servicio en la red Docker)
- **Logs Service**: `http://logs_service:8000` (comunicaci√≥n HTTP one-way)
- **Logs Path**: `/app/logs` (mapeado a `./logs_storage/scraping` para persistencia local)
- **Python Path**: `/app/src` (ra√≠z del proyecto)

#### üì¶ Componentes Internos
- **domain/models.py**: Dataclass `Vacante` con campos obligatorios y opcionales
- **application/scraper_service.py**: Orquestador que gestiona empresas y motores
- **infrastructure/adapters/**: Repositorio PostgreSQL, Logger HTTP, HiringRoom Engine
- **infrastructure/config.py**: Cargador de configuraci√≥n basado en `.env`

---

### 2Ô∏è‚É£ **Logs Service** (FastAPI)
Servicio centralizado de logging que recibe eventos HTTP del scraper y persiste registros en el almacenamiento.

#### üîó API Endpoints
- **POST `/escribir-log`**: Recibe logs asincronos del scraper
  ```json
  {
    "nombre_log": "WEB_PALMON",
    "mensaje": "[OUT] Extra√≠do: Ingeniero de Sistemas",
    "tipo": "INFO|WARNING|ERROR"
  }
  ```
- **GET `/health`**: Verifica disponibilidad del servicio
  ```json
  {
    "status": "online",
    "service": "logs-central"
  }
  ```

#### üìÅ Ubicaci√≥n de Logs
```
logs_storage/
‚îú‚îÄ‚îÄ scraping/       (Registros del scraper)
‚îú‚îÄ‚îÄ database/       (Registros de base de datos)
‚îî‚îÄ‚îÄ others/         (Otros registros)
```

---

### 3Ô∏è‚É£ **Frontend** (Next.js 16)
Interfaz de usuario React-based para visualizar y gestionar vacantes. Estado actual: configuraci√≥n m√≠nima.

#### üìù Scripts Disponibles
```bash
npm run dev    # Iniciar servidor de desarrollo (puerto 3000)
npm run build  # Compilar para producci√≥n
npm start      # Ejecutar build final
npm run lint   # Ejecutar ESLint
```

#### üé® Stack Tecnol√≥gico
- Next.js 16.1.6
- React 19.2.3
- TypeScript 5
- Tailwind CSS 4
- ESLint 9

---

### 4Ô∏è‚É£ **Database** (PostgreSQL 16)
Almacenamiento centralizado para todas las vacantes y metadatos. Inicializaci√≥n autom√°tica con `init.sql`.

#### üìä Tablas Principales
- **empresas**: Configuraci√≥n de portales a scrapear (nombre, subdomain, proveedor)
- **vacantes**: Registros de empleos (t√≠tulo, URL, ubicaci√≥n, modalidad, etc.)
- **usuario**: Usuarios del sistema
- **estado**: Estados para control de activos/inactivos

#### üîë Conexi√≥n
```
Host: database
Puerto: 5432
Usuario: ${DB_USER} (en .env)
Base de datos: ${DB_NAME} (en .env)
```

---

## ‚öôÔ∏è Comunicaci√≥n Entre Servicios

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Scraper   ‚îÇ‚îÄ‚îÄHTTP‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Logs Service ‚îÇ
‚îÇ  (Python)   ‚îÇ /escribir  ‚îÇ  (FastAPI)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚îÇ psycopg2
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   PostgreSQL 16     ‚îÇ
‚îÇ   (Database)        ‚îÇ
‚îÇ  docker network: ‚ñ∂  ‚îÇ‚óÄ todos los servicios
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîß Mantenimiento y Reseteo de Datos

### ‚ö†Ô∏è Mantenimiento B√°sico
Si realizas cambios estructurales en `init.sql` (agregar columnas, nuevas tablas), Docker no los aplicar√° autom√°ticamente porque el volumen de datos ya existe.

### üõë Limpieza Total (Reset Destructivo)
Este comando **detiene los servicios y elimina permanentemente todos los datos**:

```bash
docker-compose down -v
```

> ‚ö†Ô∏è **CUIDADO**: El flag `-v` borrar√° toda la informaci√≥n en la base de datos. √ösalo solo si quieres que `init.sql` se ejecute nuevamente desde cero.

### üîÑ Reconstrucci√≥n del Entorno
Despu√©s de la limpieza, levanta todo nuevamente para inicializar la estructura actualizada:

```bash
docker-compose up -d --build
```

---

## üåç Configuraci√≥n Global (Timezone)

**CR√çTICO**: Todos los servicios usan la zona horaria del archivo `.env`:

```yaml
TZ: ${APP_TIMEZONE:-America/Guayaquil}
PGTZ: ${APP_TIMEZONE:-America/Guayaquil}  # PostgreSQL interno
```

Modifica `APP_TIMEZONE` en `.env` si necesitas otra zona horaria.

---

## üìö Variables de Entorno (.env)

```env
# Base de datos
DB_HOST=database
DB_PORT=5432
DB_NAME=trabajos_db
DB_USER=postgres
DB_PASS=tu_contrase√±a

# Logs Service
LOGS_SERVICE_URL=http://logs_service:8000
LOGS_TIMEOUT=5

# Aplicaci√≥n
APP_TIMEZONE=America/Guayaquil
```

---

## üìñ Convenciones de Logging

Los logs siguen este formato:
- **[IN]**: Iniciando una operaci√≥n
- **[OUT]**: Datos extra√≠dos exitosamente
- **[ERROR]**: Fallos y errores

Ejemplo:
```
[IN] Iniciando scraping en Grupo Palmon
[OUT] Extra√≠do: Ingeniero de Sistemas | URL: https://...
[ERROR] Conexi√≥n rechazada por HiringRoom
```

---

## üöÄ Flujo de Ejecuci√≥n del Scraper

1. **ScraperService** obtiene lista de empresas activas de PostgreSQL
2. Para cada empresa, invoca el **motor correspondiente** (ej: HiringRoomEngine)
3. El motor extrae vacantes como objetos `Vacante`
4. **PostgresRepository** guarda/actualiza vacantes (retorna "INSERT" o "UPDATE")
5. Vacantes no encontradas se marcan como inactivas
6. **HttpLoggerAdapter** registra todas las acciones en Logs Service

---

## üîå Agregar un Nuevo Portal de Empleo

1. Crear nuevo motor en `04_TrabajosWeb_Scraper/src/infrastructure/adapters/` (ej: `workday_engine.py`)
2. Implementar la interfaz `extraer(subdominio, proveedor) ‚Üí List[Vacante]`
3. Inyectar logger en constructor:
   ```python
   def __init__(self, logger):
       self.logger = logger
   ```
4. Registrar en `main.py`:
   ```python
   motores = {
       'hiringroom': HiringRoomEngine(logger=logger),
       'workday': WorkdayEngine(logger=logger)  # Nuevo
   }
   ```
5. Agregar empresas a tabla `empresas` con `proveedor='workday'`

El servicio enrutar√° autom√°ticamente las empresas al motor correcto.

---

## üõ†Ô∏è Tecnolog√≠as Utilizadas

| Componente | Tecnolog√≠a | Versi√≥n |
|-----------|-----------|---------|
| Lenguaje (Scraper) | Python | 3.11-slim |
| Lenguaje (Logs) | Python | 3.11-slim |
| Automaci√≥n Web | Selenium | - |
| API Rest (Logs) | FastAPI | - |
| Base de Datos | PostgreSQL | 16-Alpine |
| Frontend | Next.js | 16.1.6 |
| UI Framework | React | 19.2.3 |
| Orquestaci√≥n | Docker Compose | 3.8 |

---

## ‚ùì Preguntas Frecuentes

**P: ¬øC√≥mo agrego una nueva empresa a scrapear?**
R: Inserta un registro en la tabla `empresas` con `id_estado=1` (activo) y el `proveedor` correcto.

**P: ¬øPuedo cambiar la zona horaria?**
R: S√≠, modifica `APP_TIMEZONE` en `.env` y reinicia los servicios con `docker-compose down && docker-compose up -d`.

**P: ¬øQu√© pasa si cambio `init.sql`?**
R: Debes ejecutar `docker-compose down -v` para reinicializar la BD, ya que el volumen persiste cambios autom√°ticamente.

---

**√öltima actualizaci√≥n**: Febrero 2026